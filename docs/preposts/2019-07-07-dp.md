---
layout: post-sidenav
title: "Dirichlet Process 소개 및 알고리즘 구현하기"
group: "Bayesian Statistics"
author: 임성빈
---

Nonparametric Bayesian 으로 군집화(clustering) 문제를 풀 때 사용되는 Dirichlet Process 를 소개합니다.

**[군집화(clustering)](https://en.wikipedia.org/wiki/Cluster_analysis)** 문제란 문자 그대로 **주어진 데이터를 몇 개의 부분집합(subset) 혹은 [partition](https://en.wikipedia.org/wiki/Partition_of_a_set) 으로 분할하는 것** 을 말합니다. 각 데이터 포인트 \\( x_{1}, x_{2},\ldots \\) 마다 쌍으로 정답 레이블(label) \\( y_{1}, y_{2},\ldots \\) 이 있는 [분류(classification)](https://en.wikipedia.org/wiki/Statistical_classification) 문제와는 달리 군집화 문제는 레이블이 없습니다. 그래서 [비지도 학습(unsupervised learning)](https://en.wikipedia.org/wiki/Unsupervised_learning) 에  해당합니다. 아래 그림을 보시면 왼쪽이 주어진 데이터이고, 오른쪽이 군집화 알고리즘을 거친 결과입니다.

![figure]({{ site.baseurl }}/images/posts/cluster-dp.png){:class="center-block" height="300px"}
**군집화 알고리즘을 통해 유사한 패턴을 가진 데이터끼리 모을 수 있다**


대체로 머신러닝 수업에서 배우는 군집화 알고리즘은 [K-means](https://en.wikipedia.org/wiki/K-means_clustering), [GMM (Gaussian Mixture Model)](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model), [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) 등이 있습니다. 각자가 유용하게 쓰이는 알고리즘이지만, 이번 포스트에서는 **[Nonparametric Bayesian](https://bayestour.github.io/blog/2019/07/04/npb)** 의 대표적인 방법론인 **[Dirichlet Process (DP)](https://en.wikipedia.org/wiki/Dirichlet_process)** 를 소개하겠습니다. 사실 DP 는 활용하면 군집화 문제 뿐만 아니라 다른 곳에도 응용할 수 있습니다만, 이 포스트에서는 DP 를 활용하여 군집화 문제를 푸는 방법을 소개하겠습니다.

### 군집화 문제의 Mixture 모델 표현

주어진 데이터 공간을 \\( \mathcal{X} \\) 라고 하겠습니다. 우리는 이 \\( \mathcal{X} \\) 를 쪼개는 **partition** 을 아래 그림처럼 생각해볼 수 있습니다.

![figure]({{ site.baseurl }}/images/posts/partition-dp.png){:class="center-block" height="200px"}
**데이터 공간 \\( \mathcal{X} \\) 의 partition, 출처: [최성준 박사님의 블로그](https://enginius.tistory.com/513)**

위 그림에서 집합 \\( A_{1},A_{2},\ldots, A_{K} \\) 들이 바로 데이터 공간 \\( \mathcal{X} \\) 의 partition 입니다. 군집화 문제는 관찰된 데이터로 데이터 공간에 어떻게 partition 을 구성할 것인지를 해결하는 문제로 볼 수 있습니다.  에 **이산확률분포(discrete probability distribution)** 를 구성할 수 있습니다.

우선 군집화 문제를 통계적 모델로 재해석 해보겠습니다. 군집화 문제는 **[mixture 모델](https://en.wikipedia.org/wiki/Mixture_model)** 을 사용해서 표현할 수 있습니다.

데이터가 확률변수 \\( X \\) 로 생성(혹은 관찰) 가정된다고 합시다. \\( L(X) \\) 라는 함수는 이 데이터가 \\( \mathcal{X} \\) 상의 어떤 집합에서 뽑혔는지 확인(reveal)해주는 함수라고 정의하겠습니다 [1]. 즉 \\( L(X)=k \\) 라는 건 **데이터 \\( X \\) 가 \\( k \\) 번째 집합에서 뽑혔다** 라는 의미입니다. 데이터 \\( L(X)=k \\) 일 때, \\( X \in j \\) 일 확률, 즉 데이터 \\( X \\) 가 집합 \\( j \\) 에 속한다고 우리가 (또는 알고리즘이) 판단을 내릴 확률을 \\( P_{k}(j) \\) 라 표기하겠습니다:

$$
P_{k}(j) := \mathbb{P}(X \in j | L(X)=k)
$$

올바른 모델링이라면 당연히 \\( j = k \\) 일 때 확률 \\( P_{k}(j) \\) 값이 가장 커야겠지요? 데이터 \\( X \\) 가 \\( k \\) 번째 집합에서 뽑혔을 확률을 \\( \pi_{k} := \mathbb{P}(L(X)=k) \\) 라고 표기하겠습니다. 이 확률은 mixture 모델에서 mixture weight 에 해당합니다. 즉, mixture weight 는 **데이터에서 어떤 집합이 구성하는 크기** 에 해당하는데요, 모든 \\( k \\) 에 대해 더하면 \\( \sum_{k}\pi_{k} = 1 \\) 이 성립해야 합니다. 이렇게 정의된 \\( P_{k}(j) \\) 와 \\( \pi_{k} \\) 는 조건부 확률의 정의에 의해 다음과 같이 확률분포를 구성하게 됩니다.

$$
P(j) := \mathbb{P}(X\in j) = \sum_{k}\mathbb{P}(X \in j , L(X)=k) = \sum_{k}\pi_{k}P_{k}(j)\quad \cdots\quad (1)
$$

식 (1) 을 **mixture 분포** 라 부릅니다. 즉 \\( L(X)=k \\) 라면 데이터 \\( X \\) 가 집합 \\( A_{k} \\) 에서 뽑혔다는 것과 동일한 사건입니다. 그리고 partition 의 구성원인 집합 \\( A_{k} \\) 에 부여된 이산확률분포들이 위에서 소개한 \\( \pi_{k} \\) 와 \\( P_{k}(j) \\) 입니다. 두 확률분포의 역할이 구분되어 있는 점을 기억해두세요.





앞서 [Nonparametric Bayesian][] 포스트에서 통계적 모델을 다룰 땐 모수공간(parameter space) \\( \Phi \\) 를 상정하는 것이 중요하다고 했습니다. 그렇다면 식 (1) 을 어떻게 적절한 통계적 모델로 만들 수 있을까요?





### Mixing measure 구성하기: stick-breaking

위에서 설명한 mixing measure \\(  \\)




### Clustering with DP

그렇다면 이렇게 정의한 DP 는 어떤 원리로 clustering 문제를 푸는 것일까요? DP 는 데이터공간 \\( \mathcal{X} \\) 를 분할하는

### Clustering Algorithm

이제 DP 를 이용한 clustering 알고리즘을 소개하겠습니다. 다시 강조하자면 DP 는 mixing 확률을 구성하는

### Dirichlet Process 에 대한 수학적 접근

지금까지는 직관적인 설명을 위해 다소 수학적인 부분을 배제하려고 했는데요, 이런 설명으로는 만족하지 않을 분이 계실까 염려(?)되어 수학적인 정의 및 성질에 대해서도 같이 설명하겠습니다. 단, 본 항목을 이해하려면 [측도론(measure theory)](https://en.wikipedia.org/wiki/Measure_(mathematics))의 기본적인 용어들을 알아야 합니다. 측도론을 모르시는 분들은 이 참에 어떻게 사용되는지 한 번 경험해보시는 것도 좋습니다 (물론 스킵하셔도 좋습니다).

우선 Dirichlet Process 를 수학적으로 정의하겠습니다.


그러므로 **mixing measure** 는 확률공간 \\( \mathcal{X} \\) 에 정의된  로서  \\( \theta \\) 는 다음과 같이 적분의 형태로 쓰이게 됩니다

$$
p(x) = \sum_{k}\pi_{k}p(x|\phi_{k}) = \int_{\mathcal{X}}p(x|\phi) \text{d}\theta(\phi)
$$

---

**Definition (Dirichlet Process)**

\\( \alpha > 0 \\) 이고 \\( G \\) 가 \\( \mathcal{X} \\) 위에 정의된 확률측도일 때, \\( \mathcal{X} \\) 상의 이산확률측도 \\( \Theta \\) 를 다음과 같이 정의하자:

$$
\Theta := \sum_{k} C_{k}\delta_{\Phi_{k}}, \quad \Phi_{1},\Phi_{2},\ldots,\underset{\text{i.i.d}}{\sim} Q
$$

이 때 이 \\( \Theta \\) 를 **Dirichlet Process (DP)** 라 부르고 \\( G \\) 를 **base measure**, \\( \alpha \\) 를 **concentration** 이라 한다. 여기서 \\( \delta \\) 는 [디렉 측도(Dirac measure)](https://en.wikipedia.org/wiki/Dirac_measure) 이고 \\(C_{k}\\), \\(\Phi_{k}\\) 는 다음과 같이 정의된다:

$$
\begin{aligned}
V_{1}, V_{2},\ldots \underset{\text{i.i.d}}{\sim} \text{Beta}(1,\alpha),\quad C_{k}=V_{k}\prod_{j=1}^{k-1}(1-V_{k})
\end{aligned}
$$

\\( \alpha, Q \\) 에 대해 parameter \\( \Theta \\) 의 **확률분포** 를 \\( \text{DP}(\alpha, Q) \\) 로 표기한다.

---

### Comment
[1] 실제 상황에선 비지도학습 상황이기 때문에 이런 함수를 직접 관찰할 수 없고 학습 데이터에도 주어지지 않습니다.

[2]

### 참고문헌

- *Lecture Notes on Bayesian Nonparametrics*, P. Orbanz, 2014
- *Bayesian Nonparametrics*, J.K. Ghosh, R.V. Ramamoorthi, 2003


[Nonparametric Bayesian]: https://bayestour.github.io/blog/2019/07/04/npb.html
[]
