---
layout: page-sidenav
group: "VAE"
title: "4. Reparametrization Trick"
---


## Reparametrization Trick

![figure1.1]({{ site.baseurl }}/images/agm_Figure020401.png){:class="center-block" height="300px"}

- Reparametrization trick 은 latent variable $$ z $$ 의 randomness 를 결정하는 random variable $$ \epsilon $$ 을 도입하는 것이다 :

$$ z = g(\phi,x,\epsilon) $$ 

- 가령 $$ z \sim \mathcal{N}(\mu, \sigma^{2}) $$ 를 따른다고 하자. 이 때의 variational parameter 는 $$ \phi = (\mu, \sigma) $$ 이다. 이 경우 $$ z $$ 는 다음과 같이 표현할 수 있다

$$
z \overset{d}{=} \mu + \sigma \epsilon,\quad \epsilon\sim\mathcal{N}(0,1)
$$

- 위 식에서 값이 같다는 의미에서 $$ = $$ 가 아니라 **분포가 같다** 는 의미인 $$ \overset{d}{=} $$ 으로 표현했다는 사실을 기억하자. Reparametrization trick 은 $$ z $$ 와 분포는 같지만 다른 확률변수 $$ \mu + \sigma \epsilon $$ 을 찾아서 기대값을 대신 계산하는 방법이다! $$ z $$ 는 어짜피 latent variable 이므로 ELBO 처럼 기대값을 계산할 때는 분포가 같은 확률변수를 찾는 것으로도 충분한 것이다

- 따라서 ELBO 의 기대값 계산은 $$ q_{\phi}(z\vert x) $$ 대신 $$ p(\epsilon) $$ 을 이용하여 사용할 수 있다. 이런 변환이 가능한 이유가 바로 change of variable formula 이다 :

$$
\begin{aligned}
\mathcal{L}_{\theta,\phi}(x) &= \mathbb{E}_{q_{\phi}(z\vert x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z\vert x)\right] \\
&= \int\left[\log p_{\theta}(x,z)-\log q_{\phi}(z\vert x)\right] q_{\phi}(z\vert x) dz \\
&\overset{\text{c.v.f}}{=} \int_{\Omega} \left[\log p_{\theta}(x,Z(\omega))-\log q_{\phi}(Z(\omega) \vert x) \right] \mathbb{P}(d\omega )\\
&= \int \left[\log p_{\theta}(x,g(\phi,x,\epsilon))-\log q_{\phi}(g(\phi,x,\epsilon)\vert x)\right] p(\epsilon) d\epsilon \\
&= \mathbb{E}_{p(\epsilon)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z \vert x)\right]
\end{aligned}
$$



- 위에서 적분변수가 $$ dz $$ 에서 $$ d\omega $$ 로 바뀐 후 다시 $$ d\epsilon $$ 으로 변하는 과정이 바로 measure theory 의 change of varialbe formula 이다
	- Change of variable formula 의 증명이 궁금한 사람은 본 블로그 Modern Probability Theory 의 [페이지](https://sungbinlim.github.io/sl/docs/mpt/0203)를 참고하자 
	- 보통 학부 확률론이나 미적분학에서는 여기에 Jacobian matrix 가 등장하고 함수 $$ g $$ 의 invertibility 를 요구한다. Kingma 의 논문에서도 이 점을 언급하고 있는데, 본래 저 조건은 확률밀도함수를 명시적으로(explicitly) 쓰기 위해 필요한 것이지 변수변환 자체의 필요조건은 아니다
	- 물론 실제로 컴퓨터를 통해 계산할 때는 연산이 쉬운 확률밀도함수가 필요하므로 $$ g $$ 를 굉장히 좋은 함수로 가정할 것이다

- 어쨋거나 ELBO 의 확률밀도함수를 $$ \phi $$ 에 상관없는 함수로 바꾸었으므로 이제 문제였던 두 번째 gradient $$ \nabla_{\phi} $$ 가 기대값 안으로 들어갈 수 있다 :

$$
\nabla_{\phi} \mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{p(\epsilon)}\left[\nabla_{\phi}\log p_{\theta}(x,z)-\nabla_{\phi}\log q_{\phi}(z \vert x)\right]
$$

- 그러므로 아래와 같은 몬테카를로 적분 세팅하에서 ELBO 의 gradient 를 구할 수 있다

$$
\epsilon \sim p(\epsilon)
$$

$$
z = g(\phi,x,\epsilon)
$$

$$
\tilde{\mathcal{L}}_{\theta,\phi}(x) = \log p_{\theta}(x,z) - \log q_{\phi}(z\vert x)
$$