---
layout: page-sidenav
group: "VAE"
title: "1. Introduction"
---


- [VAE (Kingma & Welling, 2014)](https://arxiv.org/abs/1312.6114) 논문 요약 설명
- [Auto-Encoder](https://en.wikipedia.org/wiki/Autoencoder) 기반 [Variational Bayes](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) 추론을 설명한다
- [Change of Variable](https://en.wikipedia.org/wiki/Integration_by_substitution) 을 이용한 Reparametrization trick 을 이해한다

### 미리 알아두면 좋은 용어들

- [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
- [Change of Variable](https://en.wikipedia.org/wiki/Integration_by_substitution)
- [Auto-Encoder](https://en.wikipedia.org/wiki/Autoencoder)
- [Variational Inference](http://norman3.github.io/prml/docs/chapter10/1)


---
## What is VAE?

- [GM](https://sungbinlim.github.io/sl/docs/agm/0) 은 기본적으로 Density estimation 작업이라 볼 수 있다 
- 데이터 $$ x $$ 가 분포 $$ x \sim p_{\text{true}}(x) $$ 를 따른다고 하자. 대부분의 경우 우리는 $$ p_{\text{\true}}(x) $$ 를 정확히 알 수 없다. 그 대신 이를 근사(approximate)하는 분포 $$ p_{\theta}(x) $$ 로 모델링해서 추정할 수 있다

$$
p_{\theta}(x)\approx p_{\text{true}}(x)
$$

- 참고로 위에서 표기한 $$ \approx $$ 는 분포 사이의 거리(distance)가 0 으로 수렴한다는 것으로 정의 내리는데, VAE 에서는 거리함수(distance function)를 [Kullback-Leibler divergence (이하 KL)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) 이다

$$
\mathbb{KL}(p_{\theta}\Vert p_{\text{true}}) = \int \log{\left(\frac{p_{\theta}(x)}{p_{\text{true}}(x)}\right)}p_{\theta}(x)dx
$$

- :smirk: 딴지걸까봐(...) 첨언하자면 KL은 거리함수는 아니지만 [pre-metric](https://en.wikipedia.org/wiki/Metric_(mathematics)#Premetrics) 이다. 그리고 [total variation (이하 TV)](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures) 으로 만들어내는 metric topology 를 만들어낼 수 있다

$$
\mathbb{KL}(P_{n}\Vert P)\to 0\quad\Longrightarrow\quad P_{n}\overset{\text{TV}}{\rightarrow} P
$$

- 문제는 $$ p_{\theta} $$ 를 어떻게 하면 유연(flexible)하면서 최적화(optimization)이 용이하게 선택하냐는 것이다. GM 의 대부분 이론은 이 부분을 공략하는데 초점을 두고 있다. 
- VAE 는 [Auto-Encoder (이하 AE)](https://en.wikipedia.org/wiki/Autoencoder) 로 유연하게, [Variational Inference (이하 VI)](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) 로 최적화한다는 철학을 가지고 있다. 한 마디로 두 마리 토끼를 모두 잡겠다는 것이다! :satisfied:
	- VI 를 속성으로 공부하고 싶은 분은 이 [링크](http://norman3.github.io/prml/docs/chapter10/1.html) 를 참조하자 (필자가 좋아하는 Bishop 책을 번역 및 요약한 페이지다)
	- AE 자료는 [최건호](https://github.com/GunhoChoi) 님의 [패스트캠퍼스 교육자료](https://github.com/GunhoChoi/PyTorch-FastCampus/blob/master/08_Autoencoder/AutoEncoder.pdf) 를 강력 추천한다!

- VAE 를 개념적으로 이해하기 쉽게 정리한 페이지는 매우 많다. 구현에 관심있는 분들은 이활석님이나 최건호님의 자료를 추천한다
	- 최건호님의 GitHub 자료 : [링크](https://github.com/GunhoChoi/PyTorch-FastCampus/blob/master/08_Autoencoder/AutoEncoder.pdf), PyTorch 구현 : [링크](https://github.com/GunhoChoi/PyTorch-FastCampus/tree/master/08_Autoencoder)
	- 강지양님의 블로그 : [링크](https://jamiekang.github.io/2017/05/21/auto-encoding-variational-bayes/) 
	- 이활석님이 정리한 자료 : [링크](http://fbsight.com/t/variational-auto-encoder/86180), TensorFlow 구현 : [링크](https://github.com/hwalsuklee/tensorflow-mnist-VAE)
	- Jaan Altosaar 의 What is variational autoencoder 를 번역한 자료 : [링크](http://nolsigan.com/blog/what-is-variational-autoencoder/)
	- 유재준님의 블로그 : [링크](http://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-1.html)
	
- 이 페이지에선 위의 내용들보다 수학적이고 Analytic 한 부분에 초점을 둘 것이다 :smirk:
- 우선 [Variational Bayesian Inference](https://sungbinlim.github.io/sl/docs/agm/0202) 가 무엇인지 살펴보자!




