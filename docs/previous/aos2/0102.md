---
layout: page-sidenav
group: "Models, Statistical Inference and Learning"
title: "2. Parametric and Nonparametric Models"
---

- **통계적 모형(statistical model)** \\( \mathfrak{F} \\) 는 분포들의 집합으로 정의한다
	- 또는 밀도함수(density function) 이나 회귀함수(regression function)들의 집합으로 본다. 보통 밀도함수는 확률분포와 대응관계까 있으므로 이 책에서는 밀도함수의 집합으로 \\( \mathfrak{F} \\) 를 정의한다

- 통계적 모형 중 parametric model 은 finite parameters 로 표현이 가능한 (parameterizable) 분포들의 집합을 말한다
	- 예 : 데이터가 정규분포에서 추출된다고 가정했을 때 \\( \mathfrak{F} \\) 를 다음과 같이 쓸 수 있다. 그러므로 \\( \mathfrak{F} \\) 는 two-parameter model 이다

$$
\mathfrak{F} = \left\{f(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right),\mu\in\mathbb{R},\sigma>0 \right\}\quad\cdots\quad (1)
$$

-	일반적으로 parametric model 은 다음과 같은 형태로 쓸 수 있다

$$
\mathfrak{F} = \{f(x;\theta) : \theta\in\Theta\}
$$

- Parameter \\( \theta \\) 는 unknown value 또는 unknown vector 로 parameter space \\( \theta \\) 에서 값을 가진다
-	- 위의 정규분포의 예에서 \\( \theta = (\mu,\sigma) \\), \\( \Theta = \mathbb{R}\times (0,\infty) \\) 이다
- Parameter 가 vector 일 때, 즉 \\( \theta = (\theta_{1},\theta_{2},\ldots,\theta_{n}) \\) 인 경우 특정성분 \\( \theta_{j} \\) 에만 관심을 가진다고 한다면, 다른 성분들 \\( \theta_{-j} \\) 을 nuisance parameters 라고 부른다

- 통계적 모형 중 Nonparametric model 은 finite parameters 로 표현이 불가능한 \\( \mathfrak{F} \\) 를 말한다. 가령 **모든 분포들** 의 집합 \\( \mathfrak{F}_{\text{ALL}} \\) 은 parametric model 이 될 수 없으므로 nonparametric model 이다 

- `Example 1` (One-dimensional Parametric Estimation) : 데이터 \\( X_{1}, X_{2},\ldots, X_{n} \\) 이 베르누이분포(\\( p \\)) 에서 독립적으로 추출되었다고 하자. 이 경우 Statistical Learning 은 parameter \\( p \\) 를 어떻게 추정할지에 대한 문제이다.

- `Example 2` (Two-dimensional Parametric Estimation) : \\( X_{1},\ldots, X_{n} \sim F \\) 이고 확률밀도함수 \\( f \in \mathfrak{F} \\) 는 (1) 의 형태라 가정하자. 이 경우 위에서 이미 언급했듯이 \\( \mathfrak{F} \\) 는 \\( \theta = (\mu,\sigma) \\) 인 two-parameter model 이다. 이 경우 Statistical Learning 은 데이터를 통해 \\( \mu,\sigma \\) 를 어떻게 추정할지에 대한 문제이다. 어떤 경우엔 \\( \mu \\) 를 추정하는 것에만 관심을 가질 수 있는데, 이 경우 \\( \sigma \\) 가 nuisance parameters 이다
- `Example 3` (Nonparametric estimation of the cdf) : 데이터 \\( X_{1},\ldots, X_{n} \\) 이 CDF \\( F \\) 를 따르는 분포에서 독립적으로 추출되었다고 가정하자. 이 때 Statistical Learning 은 \\( F \\) 를 추정하는 문제가 되는데, [Glivenko-Cantelli theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem) 에 의해 \\( \mathfrak{F}=\mathfrak{F}_{\text{ALL}} \\) 로 두기 때문에 nonparametric estimation 문제이다
- `Example 4` (Nonparametric density estimation) : 예제 3와 같은 상황에서 PDF \\( f = F' \\) 가 존재한다고 가정하자. \\( f \\) 의 추정은 예제 3과 달리 \\( \mathfrak{F} = \mathfrak{F}_{\text{ALL}} \\) 로 두는 것은 불가능하다. 이 경우 \\( f \\) 의 [smoothness](https://en.wikipedia.org/wiki/Smoothness) 에 대한 가정이 필요하다. 가령 다음과 같은 [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) 를 상정할 수 있다 (Sobolev space 는 함수들이 [지나치게 출렁이지 않는(too wiggly)](https://en.wikipedia.org/wiki/Total_variation#Total_variation_for_functions_of_one_real_variable) 함수들의 모임으로 해석가능하다)

$$
\mathfrak{F}_{\text{SOB}} =\left\{f\in L_{2}: \partial_{x}f, \partial_{x}^{2}f \in L_{2}\right\}
$$

- `Example 5` (Nonparametric estimation of functionals) : \\( X_{1},\ldots, X_{n} \sim F \\) 이라 하자. 이 때 CDF \\( F \\) 에 대해 다음과 같은 \\( F \\) 에 대한 함수인 **statistical functional** 을 추정하는 문제도 가능하다

$$
\mathbb{E}(X) = \int x dF(x),\quad \mathbb{V}(X)=\int x^{2}dF(x)-\left(\int x dF(x)\right)^{2}
$$

- `Example 6` (Regression, prediction, and classification) : 데이터가 \\( (X_{1}, Y_{1}) ,\ldots , (X_{n}, Y_{n}) \\) 으로 주어져 있다고 가정하자
	- \\( X \\) 는 predictor, regressor, feature, independent variable 등의 용어로 부른다
	- \\( Y \\) 는 outcome, response variable, dependent variable 으로 부른다
	- 주어진 $$ X=x $$ 에 대한 $$ Y $$ 의 조건부 기대값(conditional expectation)는 regression function 이라 한다 :

$$ 
r(x) = \mathbb{E}(Y | X=x)
$$

-	- 만약 $$ r $$ 을 finite-dimension $$ \mathfrak{F} $$ 로 한정짓는 경우 parametric regression model 이라 부른다. 반대로 $$ \mathfrak{F} $$ 을 infinite-dimension 으로 넓히면 nonparametric regression model 이라 한다
	- 새로운 데이터 \\( X \\) 가 주어졌을 때 \\( Y \\) 를 예측하는 것을 prediction 이라 한다. \\( Y \\) 가 이산형(discrete)인 경우 이 prediction 문제를 특별히 classification 이라 부른다.
	- 데이터 분석의 목적이 regression function \\( r \\)	을 추정하는 경우 Statistical Learning 문제는 regression 또는 curve estimation 이라 부른다. Regression 문제는 다음과 같이 쓸 수 있다

$$
Y = r(X) + \epsilon,\quad \mathbb{E}(\epsilon) = 0
$$

$$
\mathbb{E}(Y|X=x)
$$

- 통계적 추론은 Frequentist inference 와 Bayesian inference 로 나눌수 있다. 이 페이지에서는 둘 다 소개할 것이다
