---
layout: page-sidenav
group: "Chapter. 5"
title: "0. Introduction"
---

- 지금까지 우리는 3장과 4장을 통해 회귀(regression) 모형과 분류(classification) 모형을 다루어 보았다.
- 이를 처리하기 위해 고정된 기저 함수를 이용하여 선형 결합을 만들어내는 모델들을 학습했다.
- 이런 방식은 꽤나 유용하게 사용될 수 있는 기법들이지만, 차원이 증가하는 경우 차원의 저주를 막을 수 없다.
- 선형 모델에서 차원의 저주 문제는 '고정된 기저 함수'라는 가정에 기인하므로, 
- 이를 해결하기 위한 방안으로 "입력 데이터에 맞게 적응하는 기저 함수"의 도입이 필수적이다.

----

- **SVM**
    - 앞으로 나올 7장에서는 SVM 기법을 다루고 있다.
    - 이 기법은 학습 데이터로부터 모델을 학습 과정 중에 학습 데이터의 일부 데이터 집합에만 집중하여 학습을 하게 된다.
    - SVM이 가지는 장점 중 하나는 목적 함수가 convex 하기만 하면 비선형적인 함수를 통한 최적화 과정을 통해서도 결과를 도출해 낼 수 있다는 것이다.
    - 이런 경우 실제 사용되는 학습 데이터 수에 비해 기저 함수의 개수가 현격히 줄어들 수 있다.
    - 이와 관련된 내용은 7장에서 좀 더 자세히 보도록 하자.

----

- **Nueral Network (or Perceptrons)**
    - SVM 방식과는 다른 방식으로, 비록 기저 함수의 개수는 고정시키지만 좀 더 적합한 모델을 구성하는 방법도 있다.
    - 여기서는 이를 위해 "다중 레이어를 가진 신경망" (*multilayer perceptron*) 을 이용한다.
    - 이름이 너무 길다. 그냥 MLP 라고 부른다.

- 신경망은 생물학적 시스템을 모방한 시도로 피트, 로젠블렛, 럼멜하트 등 초기 많은 학자들의 노력으로 발전하였다.
    - 하지만 신경망의 역사에 대한 내용은 생략하도록 한다.

----

- 우리는 어쨌거나 기저 함수의 파라미터를 결정하는 함수적 형태로 신경망 공부를 시작할 예정이다.
- 역시나 *MLE* 등을 이용하여 문제를 해결하겠지만, 여기서는 비선형 함수를 다룬다는 특징이 있다.
- 파라미터 학습을 위해 *error backpropagation* 이라는 기법을 사용한다. 뒤에서 자세히 나온다.
- 후반부에서는 정칙화(regularization) 기법이나 혼합 모델(mixture density networks)에 관련된 내용도 다룰 것이다.
- 당연히 베이지안 방식의 신경망도 안 다룰수 없는 법.

