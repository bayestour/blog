---
layout: post-sidenav
title: "가능도 likelihood 없이 베이지안 추론을 한다고? - Likelihood-free Bayesian inference (1)"
group: "Bayesian Statistics"
author: 박준석
---

지금까지 베이지안 추론의 실제 사례들을 꽤 다루었습니다. 그리고 그 핵심은 prior와 likelihood라고 누누이 강조했습니다. 이 둘을 곱한 것이 posterior에 비례하며, 이 사실을 이용하면 MCMC 샘플링을 통해 사후분포에 대한 추론을 할 수 있다고 계속 말해왔습니다. 그런데 모든 통계모형들이 이런 방식으로 추론이 가능한 것은 아닙니다. 조금만 모형이 복잡해져도 자료의 생성 모형이 닫힌 형태의 likelihood function으로 표현될 수 없게 됩니다. 이런 모형은 불행히도 곳곳에 널려 있습니다. 특히 생명과학이나 심리학 같은 분야에서 likelihood function을 얻을 수 없는 모델들이 많이 발견됩니다. 대개 이런 모델들은 매우 복잡한 위계적 구조를 갖고 있으며, 시뮬레이션을 통해 자료를 생성할 수는 있지만 그 수학적 표현은 알려지지 않은 경우가 많습니다.

그렇다고 이런 모델들에 대해 알려진 likelihood가 없다는 이유로 (샘플링을 통한) 베이지안 추론을 할 수 없느냐? 꼭 그런 것은 아닙니다. 어떤 모델들에 대해서는 자료 시뮬레이션만 가능하면 상당히 정확한 베이지안 추론이 가능합니다. 물론 likelihood function이 수학적인 형태로 바로 가용한 경우만큼은 아니지만요. 이런 베이지안 추론 기법을 likelihood-free Bayesian inference 또는 approximate Bayesian computation (ABC) 라고 부릅니다. ABC는 모수치를 제안하는 것까지는 기존 MCMC와 비슷한데, 그것을 수락할지 말지 결정하는 방식이 좀 다릅니다. 기본 아이디어는 다음과 같습니다. 만약 어떤 모수치가 자료를 잘 설명한다면, 그 모수치 하에서 생성된 (simulate된) 자료는 실제로 관측된 자료와 상당히 닮아있을 것이고, 그렇지 않다면 그 자료는 실제 자료와 상당히 동떨어져 있을 것입니다. 그 '닮은 정도'를 어떤 방식으로 계량화할 수 있다면, 우리는 그것을 제안된 모수치를 받아들일지 말지의 지표로 삼아 샘플링을 할 수 있습니다. 그러니까 MCMC에서 제안된/현재 모수치 값의 (prior*likelihood)값의 비율을 수락 여부를 판단하는 데 사용했다면, ABC에서는 likelihood를 계산할 수 없기 때문에 대신 자료를 직접 생성해서 수락할지 판단하겠다는 것입니다. 물론 이로 인한 비효율이 발생하지만, 이는 likelihood function의 부재로 인해 어쩔 수 없이 감수해야 하는 부분입니다.

가장 간단한 ABC 샘플링 알고리즘으로는 ABC rejection sampling이라는 것이 있습니다. [1] 이 방식에서 사후분포로부터 샘플링을 하는 과정은 다음과 같습니다:

1. 사전분포에서 모수치 값을 제안한다.
2. 제안된 모수치 값을 사용하여 새로운 자료를 (모형에 따라) 생성한다.
3. 생성된 자료와 실제 관측된 자료 사이의 "거리"를 계산한다.
4-1. 계산된 "거리"가 문턱값 threshold 보다 작거나 같으면 제안된 모수치 값을 수락하여 사후분포에 저장한다.
4-2. 만약 "거리"가 문턱값보다 크면 제안된 모수치 값을 기각하고, 4-1에 도달할 때까지 1-3을 반복한다.

그러니까 ABC rejection sampling의 핵심을 말로 정리하면 "실제 관측된 자료와 충분히 비슷한 자료를 생성하는 모수치 값이 제안될 때까지 계속 반복하여, 그런 모수치가 제안되면 수락한다" 입니다. 사실 실제로 짜 보면 놀라울 정도로 매우 간단합니다. 다음 글에서는 아주 기초적인 ABC 알고리즘을 실제로 한 번 짜 보도록 하겠습니다.

참고문헌

[1] <a href="https://www.sciencedirect.com/science/article/pii/S0022249612000272
">Turner, B. M., & Van Zandt, T. (2012). A tutorial on approximate Bayesian computation. Journal of Mathematical Psychology, 56(2), 69-85.</a>
